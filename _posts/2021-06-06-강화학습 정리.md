---
layout: single
title: "강화학습 간단정리"
description: "강화학습 간단정리!"
headline: "강화학습 간단정리!"
comments: true
published: true
toc: true
toc_label: "Contents"
toc_icon: "cog"
categories:
  - Reinforcement_learning
tags:
  - machine_learning
  - python
  - Reinforcement_learning
---

# 강화학습

# 1~2강
강화학습의 예)Reinforcement learning의 예: 알파고, 알파스타, cart-pole

goal=maximize Reward하기 위한 액션들을 잘 찾는 것

reinforcement learning-맛집 찾기를 예로 들어 설명

## Q-learning 알고리즘
greedy action을 사용한다. greedy action이란 목표로 최소한으로 이동하고자 할 때 이동을 하면서 점수를 매겨서 점수가 가장 큰 쪽으로 움직이는 것을 greedy action이라고 한다.
특징: 이동을 하면서 업데이트한다.

## Exploartion
탐험을 한다. 더 좋은 최소거리를 찾기위해 탐험을 하는 것이다. 탐험을 하기 위해서
입실론-greedy를 한다. 입실론은 0과1사이의 값이다.
예를들어 입실론이 0.1이면 0.1확률만큼 greedy action와 관계 없이 아무 방향이나 가는 것이다.

## Exploration & Exploitation
탐험과 Q값을 이용한 것을 Exploitation이라고 한다.

너무 탐험만해서도 안되고 Q값만 이용해서도 안된다.

## Exploration

**장점: 1. 새로운 path를 찾을 수 있다.**
      **2. 새로운 맛집(더 좋은 목표, 전략)을 찾을 수 있다.**

## (Decaying) 입실론-Greedy
예를들어 입실론 0.9(0~1사이값)에서 0으로 줄여나가는 것이다.

## Discount factor
감마는 
1. path를 좀 더 효율적인 path를 찾아주는 역할을 한다.
감마는 0~1사이 값이다. 그 다음 것을 가져올때는 감마를 곱해준다.

2. 현재 vs 미래 reward
감마가 작을수록 미래 reward를 생각을 잘 안하게 된다.

## Q-update

![강화학습정리_수식1](https://user-images.githubusercontent.com/79041564/120922030-e07b3e00-c701-11eb-8cc5-73be9e6cdbe9.png)

알파는 0~1의 값
at를 했을 때 받는 reward를 Rt로 표기한다.


# 3강 Markov Decision Process
액션들을 쭉 진행해 나가는 것.

s_0: 시작점
액션을 하면 a_0라고 한다.

이 둘을 묶으면 s_0에서 a_0라는 액션을 하면 s_1(그 다음 statement)이다.

s_1에서 a_1을 하면 s_2가 된다.

![강화학습정리_수식2](https://user-images.githubusercontent.com/79041564/120922044-f7219500-c701-11eb-8019-93f66419cc31.png)

이 그림에서 만족되는 특성

![강화학습정리_수식3](https://user-images.githubusercontent.com/79041564/120922051-00126680-c702-11eb-9227-f23875f25cd6.png)

1.이것을 policy라고 한다. 어떤 행동을 해야할까에 대한 정책

2.이것은 transition(이동, 천이) probability라고 한다.

여기서 goal=maximize Reward에서

**정확히는 goal=maximize Expected Return이다.**


return G_t는 리워드의 합이다.


![강화학습정리_수식4](https://user-images.githubusercontent.com/79041564/120922060-0c96bf00-c702-11eb-8697-40c38faf3328.png)

# 4강 상태 가치 함수 V & 행동 가치 함수 Q & Optimal policy 개념

Expected Return을 잘 표현하는 것

## state value function 상태 가치 함수
지금부터 기대되는 return이다. 

## Action value function 행동 가치 함수 
행동에 대한 가치(점수)를 매기는 것이다. 여기서는 Q값이다.

지금 행동으로부터 기대되는 Return이다.

지금 state에서 지금 어떤 행동을 할 건데 그 행동에 대해 기대되는 return이다.

## Optimal policy
state value function을 최대화하는 것이다.

과거는 있고(과거는 놔두고) 지금(현재)부터 return되는 것을 최대화하는 것


1번은 state value function이고 2번은 Action value function이다.

![강화학습정리_수식5](https://user-images.githubusercontent.com/79041564/120922070-19b3ae00-c702-11eb-9fe8-5ca997b875ff.png)

Optimal policy은 1번을 maximize하는 
![강화학습정리_수식6](https://user-images.githubusercontent.com/79041564/120922075-1fa98f00-c702-11eb-9dae-c48806eae546.png)

을 구하는 것이다.

# 5강 Bellman equation(벨만 방정식)
강화학습에서 우리가 추구하고자 하는 목표는 value func.의 참 값을 찾는 것이 아닌 최대의 reward를 얻는 policy를 찾는 것이다.

그래서 우리는 이왕이면 가장 큰 참 값을 찾아야하는데 우리는 어떠한 목표를 이루었을 때, 최적의 상태라고 부른다. 

그래서 optimal이라는 말이 덧붙여진 단어라면 어떤 목적을 달성된 상태라고 생각할 수 있다. 

그러므로 강화학습의 목표에 따라 찾아진 policy를 우리는 optimal policy라고 부른다. 

이러한 optimal policy를 따르는 Bellman Eqn.이 바로 Bellman Optimality Equation이다.


# 6강 Optimal policy
state value function을 최대화하는 것

지금부터 기대되는 리턴을 최대화하는 것

입실론-greedy 액션을 하는 이유
![강화학습정리_수식7](https://user-images.githubusercontent.com/79041564/120922122-58e1ff00-c702-11eb-8a18-9b86b3a48212.png)


# 7강 Monte Carlo (MC)방법
Q스타를 점점 구해나가는 방법

큰수의 법칙을 사용한다.

# 8강 Temporal difference(TD) & SARSA

Incremental monte Carlo updates의 식
![강화학습정리_수식8](https://user-images.githubusercontent.com/79041564/120922140-6e572900-c702-11eb-8da1-db8bd1fc41df.png)

SARSA(state, action, Reward, next S, next A)


# 9강 MC vs TD
MC는 
장점: 1. 편향(bias)가 없다. 즉 Unbiased

단점: 2. variance가 크다.

TD는 
단점: 1. 편향이 있다. 즉 biased

장점: 2. variance가 낮다.

# 10강 On-policy vs off-policy

![강화학습정리_수식9](https://user-images.githubusercontent.com/79041564/120922150-7ca54500-c702-11eb-9ded-29298db71f06.png)


SARSA는 on-policy이다.

on-policy는 

Behavior policy와 Target policy가 같으면 on-policy이다.

만약 다르면 off-policy이다.


## Behavior policy란

![강화학습정리_수식10](https://user-images.githubusercontent.com/79041564/120922159-87f87080-c702-11eb-93ef-2e697ebe960f.png)


실제로 행동을 해서 다음 state를 얻게 되게끔 행동하게 되는 것이다.

즉 행동하는 policy이다.

Target policy
샘플을 뽑는 것이다. 

![강화학습정리_수식11](https://user-images.githubusercontent.com/79041564/120922170-90e94200-c702-11eb-9fd4-bc1cc8f66502.png)


이 target policy이다.
즉 TD target을 만들기 위한 policy이다.

## Why off-policy?

off-policy를 사용하는 이유
1. 사람이 만드는 policy나 다른 agents가 만드는 policy를 사용할 수 있다.

2. 실컷 탐험하면서 optimal policy(Greedy)로 Sample 얻는다.
여기서 sample은 TD-target이다.

3. 재평가가 가능하다. 옛날 행동에 대해서 다시 평가해볼 수 있다.


# 11강 Q-learning(심화)
보통 Behavior policy는 입실론-greedy로 하고 target policy는 Greedy action(optimal policy)를 한다.

이것이 Q-learning이 하는 것이다.




# 12강 SARSA vs Q-learning

SARSA는 reward도 전파가 된다.

Q-learning이면 최대가 되는 것을 고른다.


**이 자료는 혁펜하임 유튜브를 바탕으로 학습하여 정리한 내용입니다. 문제가 있을시 댓글이나 이메일로 연락바랍니다! 감사합니다**
https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag/featured






































